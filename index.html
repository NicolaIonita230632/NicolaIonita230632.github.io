<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gen-AI-Challenge-Info</title>
    <style>
        .github-button {
            display: inline-block;
            padding: 10px 20px;
            background-color: #4CAF50;
            color: white;
            text-align: center;
            text-decoration: none;
            border-radius: 5px;
            transition: background-color 0.3s;
        }

        .github-button:hover {
            background-color: #45a049;
        }
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #222;
            color: #ddd;
        }
        .container {
            max-width: 750px;
            margin: 0 auto;
            padding: 20px;
            background-color: #333;
            box-shadow: 0px 0px 10px rgba(255, 255, 255, 0.1);
            border-radius: 5px;
            margin-top: 20px;
        }
        h1, h2 {
            text-align: center;
            color: #ddd;
        }
        p {
            color: #bbb;
        }
        .section {
            margin-top: 30px;
        }
        .images {
            display: flex;
            justify-content: space-around;
            flex-wrap: wrap;
        }
        .image-container {
            width: 30%;
            border-radius: 5px;
            box-shadow: 0px 0px 5px rgba(255, 255, 255, 0.1);
            overflow: hidden;
            margin-bottom: 20px;
            cursor: pointer;
        }
        .image-container img {
            width: 100%;
            height: auto;
            display: block;
        }
        .popup {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background-color: rgba(0, 0, 0, 0.8);
            z-index: 999;
        }
        .popup img {
            max-width: 250%; /* 2.5 times bigger */
            max-height: 250%; /* 2.5 times bigger */
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
        }
        pre {
            background-color: #444;
            color: #ddd;
            padding: 10px;
            overflow-x: auto;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    
    <div class="container">
        <h1>Gen-AI-Challenge</h1>
        <div class="github-link">
            <a href="https://github.com/NicolaIonita230632/NicolaIonita230632.github.io" class="github-button">View on GitHub</a>
        </div>        
        <p>In this project I created a Stable Difussion XL model that I fine tuned on a dataset I created using Google Colabs</p>
        <p>During the course of the project, I faced a multitude of obstacles while endeavoring to train an AI for image generation based on the provided dataset. The tutorials I relied on turned out to be more problematic than anticipated, with two-thirds of them either demanding memberships or presenting challenges beyond my initial expectations. It was a frustrating and often confusing experience, to say the least. Despite the setbacks, I persevered in my search for a viable solution and eventually stumbled upon a tutorial that showed some promise. Although the results it yielded weren't exactly what we had hoped for, they still represented a small victory.</p>
        <div class="github-link">
            <a href="https://blog.segmind.com/finetune-sdxl-dreambooth/" style="color: #00FF00;">The tutorial I followed</a>
        </div>
        <div class="code-container">
            <pre><code>
    <span style="color: green;">//We first need to have python installed and a have access to a good GPU (15GB+ VRAM recommended)
    //Then we need to create an account on facehugger, and create a write token for us to use up ahead
    //Create the dataset and import it to a folder named images
    //Now we can start.</span>
    
    <span style="color: green">//We need to install autotrain advanced:</span>
    <span style="color: rgb(240, 128, 0);">!pip install -U -q autotrain-advanced
    
    <span style="color: green">//after that's done, we have to configure project config file</span>
    <span style="color: green;">//Here, we can set the facehugger token, project name as well as specify the model we are going to use, in our case: stable-diffusion-xl-base-1.0</span>
    
    import os
    
    //@markdown ---
    //@markdown #### Project Config
    project_name = 'Gen-AI-Challenge-Info' # @param {type:"string"}
    model_name = 'stabilityai/stable-diffusion-xl-base-1.0' # @param ["stabilityai/stable-diffusion-xl-base-1.0", "runwayml/stable-diffusion-v1-5", "stabilityai/stable-diffusion-2-1", "stabilityai/stable-diffusion-2-1-base"]
    prompt = 'ice plant, PVZ' # @param {type: "string"}
    
    //@markdown ---
    //@markdown #### Push to Hub?
    //@markdown Use these only if you want to push your trained model to a private repo in your Hugging Face Account
    //@markdown If you dont use these, the model will be saved in Google Colab and you are required to download it manually.
    //@markdown Please enter your Hugging Face write token. The trained model will be saved to your Hugging Face account.
    //@markdown You can find your token here: https://huggingface.co/settings/tokens
    push_to_hub = True # @param ["False", "True"] {type:"raw"}
    hf_token = "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" #@param {type:"string"}
    repo_id = "calypso604/Gen-AI-Challenge-Info" #@param {type:"string"}
    
    //@markdown ---
    //@markdown #### Hyperparameters
    learning_rate = 1e-4 # @param {type:"number"}
    num_steps = 250 #@param {type:"number"}  
    batch_size = 1 # @param {type:"slider", min:1, max:32, step:1}  
    gradient_accumulation = 2 # @param {type:"slider", min:1, max:32, step:1}  
    resolution = 512 # @param {type:"slider", min:128, max:1024, step:128}  
    use_8bit_adam = False # @param ["False", "True"] {type:"raw"}
    use_xformers = False # @param ["False", "True"] {type:"raw"}
    use_fp16 = True # @param ["False", "True"] {type:"raw"}
    train_text_encoder = False # @param ["False", "True"] {type:"raw"}
    gradient_checkpointing = True # @param ["False", "True"] {type:"raw"}
    
    os.environ["PROJECT_NAME"] = project_name
    os.environ["MODEL_NAME"] = model_name
    os.environ["PROMPT"] = prompt
    os.environ["PUSH_TO_HUB"] = str(push_to_hub)
    os.environ["HF_TOKEN"] = hf_token
    os.environ["REPO_ID"] = repo_id
    os.environ["LEARNING_RATE"] = str(learning_rate)
    os.environ["NUM_STEPS"] = str(num_steps)
    os.environ["BATCH_SIZE"] = str(batch_size)
    os.environ["GRADIENT_ACCUMULATION"] = str(gradient_accumulation)
    os.environ["RESOLUTION"] = str(resolution)
    os.environ["USE_8BIT_ADAM"] = str(use_8bit_adam)
    os.environ["USE_XFORMERS"] = str(use_xformers)
    os.environ["USE_FP16"] = str(use_fp16)
    os.environ["TRAIN_TEXT_ENCODER"] = str(train_text_encoder)
    os.environ["GRADIENT_CHECKPOINTING"] = str(gradient_checkpointing)
    
    <span style="color: green;">//After that, we have to configure the parameters relating to the batch size, resolution etc. then start the autotrainer:</span>
    
    !autotrain dreambooth \
    --model ${MODEL_NAME} \
    --project-name ${PROJECT_NAME} \
    --image-path images/ \
    --prompt "${PROMPT}" \
    --resolution ${RESOLUTION} \
    --batch-size ${BATCH_SIZE} \
    --num-steps ${NUM_STEPS} \
    --lr ${LEARNING_RATE} \
    $( [[ "$USE_FP16" == "True" ]] && echo "--fp16" ) \
    $( [[ "$USE_XFORMERS" == "True" ]] && echo "--xformers" ) \
    $( [[ "$TRAIN_TEXT_ENCODER" == "True" ]] && echo "--train-text-encoder" ) \
    $( [[ "$USE_8BIT_ADAM" == "True" ]] && echo "--use-8bit-adam" ) \
    $( [[ "$PUSH_TO_HUB" == "True" ]] && echo "--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}" )
    
    <span style="color: green;">//in my case. i had to delete the --gradient-accumulation as the main program didnt see it as a possible argument but this may vary on your hardware/software and instalation.</span>
    <span style="color: green;">//finally, we can run the inference code, that lets us compile everything.</span>
    
    from diffusers import DiffusionPipeline
    import torch
    from diffusers import DiffusionPipeline, AutoencoderKL
    vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
    pipe = DiffusionPipeline.from_pretrained(
        "stabilityai/stable-diffusion-xl-base-1.0",
        vae=vae,
        torch_dtype=torch.float16,
        variant="fp16",
        use_safetensors=True
    )
    pipe.load_lora_weights("dhanushreddy29/sdxl-dreambooth-shirts")
    pipe.to("cuda")
    
    <span style="color: green;">//in the final step we can run a prompt command, that tells the program to start generating images based on prompts and parameters</span>
    
    prompt = "Imagine an Ice Peashooter"
    neg_prompt = "out of frame, lowres, text, error, cropped, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, out of frame, poorly drawn face, mutation, deformed, blurry, bad anatomy, bad proportions, cloned face, disfigured"
    pipe(
        prompt=prompt,
        negative_prompt=neg_prompt,
        guidance_scale=12.5,
        num_inference_steps=20,
        num_images_per_prompt=2,
        generator=torch.manual_seed(128)
    ).images[0]
                            
                </code></pre>

        </div>
        <div class="section">
            <h2>Here is the dataset I used</h2>
            <div class="images">
                <div class="image-container" onclick="openPopup('dataset/dataset_image1.jpg')">
                    <img src="dataset/dataset_image1.jpg" alt="Dataset Image 1">
                </div>
                <div class="image-container" onclick="openPopup('dataset/dataset_image2.jpg')">
                    <img src="dataset/dataset_image2.jpg" alt="Dataset Image 2">
                </div>
                <div class="image-container" onclick="openPopup('dataset/dataset_image3.png')">
                    <img src="dataset/dataset_image3.png" alt="Dataset Image 3">
                </div>
                <div class="image-container" onclick="openPopup('dataset/dataset_image4.jpg')">
                    <img src="dataset/dataset_image4.jpg" alt="Dataset Image 4">
                </div>
                <div class="image-container" onclick="openPopup('dataset/dataset_image5.jpg')">
                    <img src="dataset/dataset_image5.jpg" alt="Dataset Image 5">
                </div>
            </div>
        </div>
        
        <div class="section">
            <h2>And here are the results</h2>
            <div class="images">
                <div class="image-container" onclick="openPopup('results/rez1.png')">
                    <img src="results/rez1.png" alt="Result Image 1">
                </div>
                <div class="image-container" onclick="openPopup('results/rez2.png')">
                    <img src="results/rez2.png" alt="Result Image 2">
                </div>
                <div class="image-container" onclick="openPopup('results/rez3.png')">
                    <img src="results/rez3.png" alt="Result Image 3">
                </div>
                <div class="image-container" onclick="openPopup('results/rez4.png')">
                    <img src="results/rez4.png" alt="Result Image 4">
                </div>
                <div class="image-container" onclick="openPopup('results/rez5.png')">
                    <img src="results/rez5.png" alt="Result Image 5">
                </div>
            </div>
        </div>
        <p>As you can see, the results aren't that closely related to the pictures in the dataset. I tried other options, including the JupyterLab
environment. The problem i faced was that any Pytorch drivers i installed couldn't see A6000 GPU, even if I installed it with the package coresponding to the Cuda drivers running the A6000. (12.2)</p>
    </div>

    <div class="popup" id="imagePopup" onclick="closePopup()">
        <img id="popupImage" src="" alt="Popup Image">
    </div>


    <script>
        function openPopup(imageSrc) {
            var popup = document.getElementById('imagePopup');
            var popupImage = document.getElementById('popupImage');
            popupImage.src = imageSrc;
            popup.style.display = 'block';
        }

        function closePopup() {
            var popup = document.getElementById('imagePopup');
            popup.style.display = 'none';
        }
    </script>
</body>
</html>
